{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3219cb5",
   "metadata": {},
   "source": [
    "# Setup and Imports\n",
    "\n",
    "Install required libraries and import modules for dataset processing, model training, and app functionality.\n",
    "\n",
    "**Note**: Run `!pip install ...` in Colab or ensure dependencies are installed locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow opencv-python numpy pandas scikit-learn matplotlib pygame mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data processing and model training\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Import libraries for app functionality\n",
    "import random\n",
    "import os\n",
    "import pygame\n",
    "import time\n",
    "from tkinter import Tk, Label, Button, StringVar, Scale, HORIZONTAL, Frame\n",
    "from threading import Thread\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "\n",
    "# Ensure matplotlib works in Jupyter\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498525fc",
   "metadata": {},
   "source": [
    "# Load FER2013 Dataset\n",
    "\n",
    "Loads the FER2013 dataset from `data/fer2013.csv`, containing ~35,887 grayscale 48x48 images labeled with 7 emotions (Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral).\n",
    "\n",
    "**Ensure**: `data/fer2013.csv` is in place. In Colab, upload or use Kaggle API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb54eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_fer2013_chunked(data_path='data/fer2013.csv', chunksize=5000):\n",
    "    \"\"\"Load FER2013 dataset in chunks to reduce memory usage.\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    try:\n",
    "        # Read CSV in chunks\n",
    "        for chunk in pd.read_csv(data_path, chunksize=chunksize):\n",
    "            # Process pixels\n",
    "            chunk_pixels = chunk['pixels'].apply(lambda x: np.array(x.split(), dtype='float32'))\n",
    "            chunk_images = np.array([pixel.reshape(48, 48, 1) for pixel in chunk_pixels]) / 255.0  # Normalize\n",
    "            chunk_labels = chunk['emotion'].values\n",
    "\n",
    "            # Append to lists\n",
    "            images.append(chunk_images)\n",
    "            labels.append(chunk_labels)\n",
    "\n",
    "            print(f\"Processed chunk of {len(chunk)} images. Total: {len(np.concatenate(labels))}\")\n",
    "\n",
    "        # Concatenate all chunks\n",
    "        images = np.concatenate(images, axis=0)\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "        print(f\"Loaded {len(images)} images with shape {images.shape}\")\n",
    "        return images, labels\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {data_path} not found. Please download from Kaggle.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load dataset\n",
    "images, labels = load_fer2013_chunked()\n",
    "if images is None:\n",
    "    raise SystemExit(\"Dataset loading failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940b694",
   "metadata": {},
   "source": [
    "# Preprocess Data and Setup Augmentation\n",
    "\n",
    "Splits data into training and validation sets, applies one-hot encoding, and configures data augmentation to improve model robustness (rotation, zoom, flips)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae442619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(images, labels):\n",
    "    \"\"\"Split data and one-hot encode labels.\"\"\"\n",
    "    # Split into train (80%) and validation (20%)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        images, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "    )\n",
    "    # One-hot encode labels\n",
    "    lb = LabelBinarizer()\n",
    "    y_train = lb.fit_transform(y_train)\n",
    "    y_val = lb.transform(y_val)\n",
    "    print(f\"Train: {X_train.shape}, Validation: {X_val.shape}\")\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "# Preprocess data\n",
    "X_train, X_val, y_train, y_val = preprocess_data(images, labels)\n",
    "\n",
    "# Setup data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Visualize augmentation (optional)\n",
    "sample_image = X_train[0].reshape(48, 48)\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.imshow(sample_image, cmap='gray')\n",
    "plt.title(\"Original\")\n",
    "for i in range(4):\n",
    "    aug_iter = datagen.flow(X_train[0:1], batch_size=1)\n",
    "    aug_image = next(aug_iter)[0].reshape(48, 48)\n",
    "    plt.subplot(1, 5, i+2)\n",
    "    plt.imshow(aug_image, cmap='gray')\n",
    "    plt.title(f\"Augmented {i+1}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e415f",
   "metadata": {},
   "source": [
    "# Build and Train CNN Model\n",
    "\n",
    "Defines a CNN architecture with augmentation and trains it on FER2013. Saves the best model to `models/emotion_model.h5`.\n",
    "\n",
    "**Note**: Training may take ~20 min on GPU, longer on CPU. Use Colab for faster results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9de095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"Define CNN architecture for emotion detection.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(48, 48, 1)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(7, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Create models directory\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# Train model\n",
    "model = build_model()\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint('models/emotion_model.h5', save_best_only=True, monitor='val_accuracy', save_weights_only=False)\n",
    "]\n",
    "\n",
    "# Assume X_train, y_train, X_val, y_val, datagen are defined from previous cells\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=64),\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Save model with optimizer state\n",
    "model.save('models/emotion_model.h5', include_optimizer=True)\n",
    "\n",
    "print(\"Model saved to models/emotion_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54e59e9",
   "metadata": {},
   "source": [
    "# Custom Emotion Detection Function\n",
    "\n",
    "Defines a function to detect emotions from webcam frames using the trained model. Replaces DeepFace in the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a701ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def preprocess_frame(frame, target_size=(48, 48)):\n",
    "    \"\"\"Preprocess a frame for emotion detection with robust face detection.\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # Initialize MediaPipe face detection\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "    with mp_face_detection.FaceDetection(min_detection_confidence=0.5) as face_detection:\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(rgb_frame)\n",
    "        if results.detections:\n",
    "            detection = results.detections[0]\n",
    "            bbox = detection.location_data.relative_bounding_box\n",
    "            # Convert relative to absolute coordinates\n",
    "            x = int(bbox.xmin * w)\n",
    "            y = int(bbox.ymin * h)\n",
    "            width = int(bbox.width * w)\n",
    "            height = int(bbox.height * h)\n",
    "\n",
    "            # Validate and clip coordinates\n",
    "            x = max(0, min(x, w-1))\n",
    "            y = max(0, min(y, h-1))\n",
    "            width = max(1, min(width, w-x))\n",
    "            height = max(1, min(height, h-y))\n",
    "\n",
    "            print(f\"Debug: MediaPipe bbox - x: {x}, y: {y}, width: {width}, height: {height}\")\n",
    "\n",
    "            # Extract face region\n",
    "            try:\n",
    "                face = gray[y:y+height, x:x+width]\n",
    "                if face.size == 0:\n",
    "                    print(\"Debug: Face region is empty after extraction.\")\n",
    "                    return None\n",
    "                face = cv2.resize(face, target_size)\n",
    "                face = face.astype('float32') / 255.0\n",
    "                face = face.reshape(1, 48, 48, 1)\n",
    "                print(\"Debug: Face preprocessed successfully.\")\n",
    "                return face\n",
    "            except Exception as e:\n",
    "                print(f\"Debug: Error preprocessing face: {e}\")\n",
    "                return None\n",
    "        else:\n",
    "            # Try Haar Cascade as fallback\n",
    "            print(\"Debug: No face detected with MediaPipe. Trying Haar...\")\n",
    "            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=3, minSize=(30, 30))\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                print(\"Debug: Face detected with Haar Cascade.\")\n",
    "                (x, y, w, h) = faces[0]\n",
    "                try:\n",
    "                    face = gray[y:y+h, x:x+w]\n",
    "                    if face.size == 0:\n",
    "                        print(\"Debug: Haar face region is empty.\")\n",
    "                        return None\n",
    "                    face = cv2.resize(face, target_size)\n",
    "                    face = face.astype('float32') / 255.0\n",
    "                    face = face.reshape(1, 48, 48, 1)\n",
    "                    print(\"Debug: Face preprocessed successfully.\")\n",
    "                    return face\n",
    "                except Exception as e:\n",
    "                    print(f\"Debug: Error preprocessing Haar face: {e}\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(\"Debug: No faces detected by either method.\")\n",
    "                return None\n",
    "\n",
    "def detect_emotion_with_custom_model(frame):\n",
    "    \"\"\"Detect emotion using the custom model.\"\"\"\n",
    "    try:\n",
    "        model = load_model('models/emotion_model.h5')\n",
    "        # Compile as fallback if model wasn't saved with optimizer\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "        processed_frame = preprocess_frame(frame)\n",
    "        if processed_frame is None:\n",
    "            return \"Neutral\"\n",
    "\n",
    "        prediction = model.predict(processed_frame, verbose=0)\n",
    "        probabilities = prediction[0]\n",
    "        dominant_emotion = emotion_labels[np.argmax(probabilities)]\n",
    "        print(f\"Debug: Probabilities: {probabilities}\")\n",
    "        print(f\"Debug: Predicted emotion: {dominant_emotion}\")\n",
    "\n",
    "        return dominant_emotion\n",
    "    except Exception as e:\n",
    "        print(f\"Custom model error: {e}\")\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Test the function with enhanced debugging\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open webcam. Try index 1 or check connection.\")\n",
    "else:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        print(\"Debug: Webcam frame captured successfully.\")\n",
    "        mood = detect_emotion_with_custom_model(frame)\n",
    "        print(f\"Test detection: {mood}\")\n",
    "    else:\n",
    "        print(\"Error: Failed to capture frame from webcam.\")\n",
    "    cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f8ff5",
   "metadata": {},
   "source": [
    "# Mood-Based Music Player with Custom Model\n",
    "\n",
    "Integrates the custom emotion detection model into your gesture-controlled music player. Replaces DeepFace while preserving all functionality.\n",
    "\n",
    "**Note**: Update `song_folder` if running in Colab (e.g., mount Google Drive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8d3889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "song_folder = r\"C:\\Users\\Ayush\\Downloads\\songs\"  # Update for Colab if needed\n",
    "\n",
    "# Global variables\n",
    "current_song = None\n",
    "detected_mood = \"Neutral\"\n",
    "music_playing = False\n",
    "music_paused = False\n",
    "exit_flag = False\n",
    "current_volume = 0.5\n",
    "gesture_detection_enabled = True\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.6\n",
    ")\n",
    "\n",
    "# Initialize pygame mixer\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Emotion labels\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Face classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Gesture tracking variables\n",
    "prev_hand_positions = deque(maxlen=10)\n",
    "swipe_threshold = 0.15\n",
    "pinch_distances = deque(maxlen=5)\n",
    "last_gesture_time = 0\n",
    "gesture_cooldown = 0.8\n",
    "current_gesture = \"None\"\n",
    "debug_info = {}\n",
    "\n",
    "class GestureState:\n",
    "    def __init__(self):\n",
    "        self.swipe_start_x = None\n",
    "        self.swipe_in_progress = False\n",
    "        self.palm_shown = False\n",
    "        self.thumbs_up_shown = False\n",
    "        self.fist_shown = False\n",
    "        self.pinch_reference = None\n",
    "        self.last_recognized_gesture = None\n",
    "\n",
    "gesture_state = GestureState()\n",
    "\n",
    "def detect_mood():\n",
    "    global detected_mood\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    status_var.set(\"Detecting Mood...\")\n",
    "    mood_var.set(\"Detecting...\")\n",
    "    update_mood_color(\"gray\")\n",
    "\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as face_mesh:\n",
    "\n",
    "        start_time = time.time()\n",
    "        mood_counts = {}\n",
    "\n",
    "        while time.time() - start_time < 5:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            rgb_frame.flags.writeable = False\n",
    "            results = face_mesh.process(rgb_frame)\n",
    "            rgb_frame.flags.writeable = True\n",
    "            frame = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            current_mood = detect_emotion_with_custom_model(frame)\n",
    "            if current_mood in mood_counts:\n",
    "                mood_counts[current_mood] += 1\n",
    "            else:\n",
    "                mood_counts[current_mood] = 1\n",
    "\n",
    "            detected_mood = current_mood\n",
    "            mood_var.set(f\"Mood: {detected_mood}\")\n",
    "            update_mood_color(detected_mood)\n",
    "\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "            for (x, y, w, h) in faces:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "            cv2.putText(frame, f\"Detecting mood: {detected_mood}\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "            progress = min(100, int((time.time() - start_time) / 5 * 100))\n",
    "            cv2.rectangle(frame, (10, 50), (10 + int(progress * 2), 70), (0, 255, 0), -1)\n",
    "            cv2.rectangle(frame, (10, 50), (210, 70), (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f\"{progress}%\", (215, 65),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "            cv2.imshow(\"Mood Detector\", frame)\n",
    "            if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        if mood_counts:\n",
    "            detected_mood = max(mood_counts, key=mood_counts.get)\n",
    "\n",
    "        status_var.set(f\"Mood detected: {detected_mood}\")\n",
    "        print(f\"Final Mood Detected: {detected_mood}\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def calculate_hand_features(landmarks, frame_width, frame_height):\n",
    "    wrist = landmarks[0]\n",
    "    thumb_tip = landmarks[4]\n",
    "    index_tip = landmarks[8]\n",
    "    middle_tip = landmarks[12]\n",
    "    ring_tip = landmarks[16]\n",
    "    pinky_tip = landmarks[20]\n",
    "\n",
    "    palm_points = [landmarks[0], landmarks[5], landmarks[9], landmarks[13], landmarks[17]]\n",
    "    palm_x = sum(point.x for point in palm_points) / len(palm_points)\n",
    "    palm_y = sum(point.y for point in palm_points) / len(palm_points)\n",
    "    palm_z = sum(point.z for point in palm_points) / len(palm_points)\n",
    "\n",
    "    thumb_tip_coord = (thumb_tip.x * frame_width, thumb_tip.y * frame_height)\n",
    "    index_tip_coord = (index_tip.x * frame_width, index_tip.y * frame_height)\n",
    "    palm_center = (palm_x * frame_width, palm_y * frame_height)\n",
    "\n",
    "    pinch_distance = ((thumb_tip.x - index_tip.x) ** 2 +\n",
    "                      (thumb_tip.y - index_tip.y) ** 2) ** 0.5\n",
    "\n",
    "    fingertips_to_palm = [\n",
    "        ((tip.x - palm_x) ** 2 + (tip.y - palm_y) ** 2) ** 0.5\n",
    "        for tip in [thumb_tip, index_tip, middle_tip, ring_tip, pinky_tip]\n",
    "    ]\n",
    "\n",
    "    finger_heights = [\n",
    "        wrist.y - tip.y\n",
    "        for tip in [thumb_tip, index_tip, middle_tip, ring_tip, pinky_tip]\n",
    "    ]\n",
    "\n",
    "    palm_facing_camera = (middle_tip.z - wrist.z) < -0.05\n",
    "    thumb_up = thumb_tip.y < wrist.y - 0.1\n",
    "    avg_fingertip_z = sum(tip.z for tip in [thumb_tip, index_tip, middle_tip, ring_tip, pinky_tip]) / 5\n",
    "\n",
    "    finger_curl_thresholds = [0.08, 0.06, 0.06, 0.06]\n",
    "    fingers_curled = [\n",
    "        abs(landmarks[4*i+3].y - landmarks[4*i+1].y) < finger_curl_thresholds[i-1]\n",
    "        for i in range(1, 5)\n",
    "    ]\n",
    "    fist_detected = all(fingers_curled) and thumb_tip.x > index_tip.x\n",
    "\n",
    "    return {\n",
    "        \"palm_center\": palm_center,\n",
    "        \"pinch_distance\": pinch_distance,\n",
    "        \"palm_facing_camera\": palm_facing_camera,\n",
    "        \"thumb_up\": thumb_up,\n",
    "        \"fingertips_to_palm\": fingertips_to_palm,\n",
    "        \"finger_heights\": finger_heights,\n",
    "        \"fist_detected\": fist_detected,\n",
    "        \"thumb_tip\": thumb_tip_coord,\n",
    "        \"index_tip\": index_tip_coord,\n",
    "        \"avg_fingertip_z\": avg_fingertip_z\n",
    "    }\n",
    "\n",
    "def detect_gestures(hand_features, frame_shape):\n",
    "    global gesture_state, current_gesture, last_gesture_time, debug_info\n",
    "    current_time = time.time()\n",
    "    width, height = frame_shape[1], frame_shape[0]\n",
    "\n",
    "    palm_center = hand_features[\"palm_center\"]\n",
    "    prev_hand_positions.append(palm_center)\n",
    "    current_pinch = hand_features[\"pinch_distance\"]\n",
    "    pinch_distances.append(current_pinch)\n",
    "    current_gesture = \"None\"\n",
    "\n",
    "    debug_info = {\n",
    "        \"palm_facing\": hand_features[\"palm_facing_camera\"],\n",
    "        \"fist\": hand_features[\"fist_detected\"],\n",
    "        \"thumb_up\": hand_features[\"thumb_up\"],\n",
    "        \"pinch\": round(current_pinch, 3)\n",
    "    }\n",
    "\n",
    "    if current_time - last_gesture_time < gesture_cooldown:\n",
    "        return None\n",
    "\n",
    "    if hand_features[\"palm_facing_camera\"] and not gesture_state.palm_shown:\n",
    "        current_gesture = \"Palm Stop\"\n",
    "        gesture_state.palm_shown = True\n",
    "        last_gesture_time = current_time\n",
    "        return \"palm_stop\"\n",
    "\n",
    "    if not hand_features[\"palm_facing_camera\"] and gesture_state.palm_shown:\n",
    "        gesture_state.palm_shown = False\n",
    "\n",
    "    if hand_features[\"thumb_up\"] and not hand_features[\"fist_detected\"] and not gesture_state.thumbs_up_shown:\n",
    "        current_gesture = \"Thumbs Up\"\n",
    "        gesture_state.thumbs_up_shown = True\n",
    "        last_gesture_time = current_time\n",
    "        return \"thumbs_up\"\n",
    "\n",
    "    if not hand_features[\"thumb_up\"] and gesture_state.thumbs_up_shown:\n",
    "        gesture_state.thumbs_up_shown = False\n",
    "\n",
    "    if hand_features[\"fist_detected\"] and not gesture_state.fist_shown:\n",
    "        current_gesture = \"Fist\"\n",
    "        gesture_state.fist_shown = True\n",
    "        last_gesture_time = current_time\n",
    "        return \"fist\"\n",
    "\n",
    "    if not hand_features[\"fist_detected\"] and gesture_state.fist_shown:\n",
    "        gesture_state.fist_shown = False\n",
    "\n",
    "    if len(prev_hand_positions) >= 5:\n",
    "        oldest_pos = prev_hand_positions[0]\n",
    "        newest_pos = prev_hand_positions[-1]\n",
    "        x_distance = (newest_pos[0] - oldest_pos[0]) / width\n",
    "        if abs(x_distance) > swipe_threshold:\n",
    "            prev_hand_positions.clear()\n",
    "            if x_distance > 0:\n",
    "                current_gesture = \"Swipe Right\"\n",
    "                last_gesture_time = current_time\n",
    "                return \"swipe_right\"\n",
    "            else:\n",
    "                current_gesture = \"Swipe Left\"\n",
    "                last_gesture_time = current_time\n",
    "                return \"swipe_left\"\n",
    "\n",
    "    if len(pinch_distances) >= 3:\n",
    "        pinch_list = list(pinch_distances)\n",
    "        avg_current = sum(pinch_list[-2:]) / 2\n",
    "        avg_previous = sum(pinch_list[:-2]) / (len(pinch_list) - 2)\n",
    "        if abs(avg_current - avg_previous) > 0.02:\n",
    "            last_gesture_time = current_time - gesture_cooldown/2\n",
    "            if avg_current < avg_previous:\n",
    "                current_gesture = \"Pinch In\"\n",
    "                return \"pinch_in\"\n",
    "            else:\n",
    "                current_gesture = \"Pinch Out\"\n",
    "                return \"pinch_out\"\n",
    "\n",
    "    return None\n",
    "\n",
    "def process_hand_gestures():\n",
    "    global prev_hand_positions, pinch_distances, last_gesture_time, exit_flag, current_gesture\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    frame_count = 0\n",
    "    fps_start_time = time.time()\n",
    "    fps = 0\n",
    "\n",
    "    while not exit_flag:\n",
    "        if not gesture_detection_enabled:\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        frame_count += 1\n",
    "        if (time.time() - fps_start_time) > 1:\n",
    "            fps = frame_count / (time.time() - fps_start_time)\n",
    "            frame_count = 0\n",
    "            fps_start_time = time.time()\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            hand_features = calculate_hand_features(\n",
    "                hand_landmarks.landmark,\n",
    "                frame.shape[1],\n",
    "                frame.shape[0]\n",
    "            )\n",
    "\n",
    "            detected_gesture = detect_gestures(hand_features, frame.shape)\n",
    "            if detected_gesture:\n",
    "                if detected_gesture == \"palm_stop\":\n",
    "                    status_var.set(\"Gesture: Palm ‚Üí Stop Music\")\n",
    "                    stop_song()\n",
    "                elif detected_gesture == \"thumbs_up\":\n",
    "                    status_var.set(\"Gesture: Thumbs Up ‚Üí Play Music\")\n",
    "                    if not music_playing:\n",
    "                        play_song(detected_mood)\n",
    "                    elif music_paused:\n",
    "                        toggle_pause()\n",
    "                elif detected_gesture == \"fist\":\n",
    "                    status_var.set(\"Gesture: Fist ‚Üí Pause/Resume\")\n",
    "                    toggle_pause()\n",
    "                elif detected_gesture == \"swipe_right\":\n",
    "                    status_var.set(\"Gesture: Swipe Right ‚Üí Next Song\")\n",
    "                    next_song()\n",
    "                elif detected_gesture == \"swipe_left\":\n",
    "                    status_var.set(\"Gesture: Swipe Left ‚Üí Previous Song\")\n",
    "                elif detected_gesture == \"pinch_in\":\n",
    "                    status_var.set(\"Gesture: Pinch In ‚Üí Volume Down\")\n",
    "                    adjust_volume(-0.05)\n",
    "                elif detected_gesture == \"pinch_out\":\n",
    "                    status_var.set(\"Gesture: Pinch Out ‚Üí Volume Up\")\n",
    "                    adjust_volume(0.05)\n",
    "\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "\n",
    "            if hand_features[\"thumb_tip\"] and hand_features[\"index_tip\"]:\n",
    "                cv2.line(frame,\n",
    "                         (int(hand_features[\"thumb_tip\"][0]), int(hand_features[\"thumb_tip\"][1])),\n",
    "                         (int(hand_features[\"index_tip\"][0]), int(hand_features[\"index_tip\"][1])),\n",
    "                         (0, 255, 255), 2)\n",
    "\n",
    "        cv2.putText(frame, f\"Mood: {detected_mood}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, f\"Current Gesture: {current_gesture}\", (10, 60),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "\n",
    "        if music_playing:\n",
    "            play_status = \"PLAYING\" if not music_paused else \"PAUSED\"\n",
    "            cv2.putText(frame, f\"{play_status}: {current_song}\", (10, 90),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            volume_percent = int(current_volume * 100)\n",
    "            cv2.putText(frame, f\"Volume: {volume_percent}%\", (10, 120),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            cv2.rectangle(frame, (150, 110), (150 + volume_percent, 125), (0, 255, 0), -1)\n",
    "\n",
    "        y_pos = 150\n",
    "        for key, value in debug_info.items():\n",
    "            cv2.putText(frame, f\"{key}: {value}\", (10, y_pos),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "            y_pos += 25\n",
    "\n",
    "        cv2.putText(frame, f\"FPS: {int(fps)}\", (frame.shape[1] - 100, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "\n",
    "        cv2.rectangle(frame, (frame.shape[1] - 210, 50), (frame.shape[1] - 10, 250), (0, 0, 0), -1)\n",
    "        cv2.rectangle(frame, (frame.shape[1] - 210, 50), (frame.shape[1] - 10, 250), (255, 255, 255), 2)\n",
    "\n",
    "        guide_text = [\n",
    "            \"Gesture Controls:\",\n",
    "            \"Palm: Stop Music\",\n",
    "            \"Thumbs Up: Play Music\",\n",
    "            \"Fist: Pause/Resume\",\n",
    "            \"Swipe Left/Right: Change Song\",\n",
    "            \"Pinch In/Out: Volume Control\"\n",
    "        ]\n",
    "\n",
    "        for i, text in enumerate(guide_text):\n",
    "            cv2.putText(frame, text, (frame.shape[1] - 200, 80 + i * 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "        cv2.imshow(\"Hand Gesture Controls\", frame)\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def update_mood_color(mood):\n",
    "    mood_colors = {\n",
    "        \"Happy\": \"green\",\n",
    "        \"Sad\": \"blue\",\n",
    "        \"Neutral\": \"gray\",\n",
    "        \"Angry\": \"red\",\n",
    "        \"Fear\": \"purple\",\n",
    "        \"Disgust\": \"brown\",\n",
    "        \"Surprise\": \"orange\"\n",
    "    }\n",
    "    mood_label.config(fg=mood_colors.get(mood, \"black\"))\n",
    "\n",
    "def get_song_list_for_mood(mood):\n",
    "    songs = [file for file in os.listdir(song_folder) if mood.lower() in file.lower()]\n",
    "    if not songs:\n",
    "        songs = os.listdir(song_folder)\n",
    "    return songs\n",
    "\n",
    "def play_song(mood):\n",
    "    global current_song, music_playing, music_paused, current_volume\n",
    "    songs = get_song_list_for_mood(mood)\n",
    "    if not songs:\n",
    "        status_var.set(f\"No songs found in {song_folder}\")\n",
    "        return\n",
    "\n",
    "    current_song = random.choice(songs)\n",
    "    song_path = os.path.join(song_folder, current_song)\n",
    "    try:\n",
    "        pygame.mixer.music.load(song_path)\n",
    "        pygame.mixer.music.set_volume(current_volume)\n",
    "        pygame.mixer.music.play()\n",
    "        music_playing = True\n",
    "        music_paused = False\n",
    "        song_var.set(f\"Playing: {current_song}\")\n",
    "        status_var.set(\"üéµ Enjoy your music!\")\n",
    "        volume_var.set(str(int(current_volume * 100)))\n",
    "        volume_scale.set(int(current_volume * 100))\n",
    "    except Exception as e:\n",
    "        status_var.set(f\"Error playing song: {e}\")\n",
    "\n",
    "def stop_song():\n",
    "    global music_playing, music_paused\n",
    "    pygame.mixer.music.stop()\n",
    "    music_playing = False\n",
    "    music_paused = False\n",
    "    song_var.set(\"Music Stopped\")\n",
    "    status_var.set(\"‚èπ Song stopped.\")\n",
    "\n",
    "def next_song():\n",
    "    if detected_mood != \"Neutral\":\n",
    "        stop_song()\n",
    "        time.sleep(0.5)\n",
    "        play_song(detected_mood)\n",
    "    else:\n",
    "        status_var.set(\"Detect mood first\")\n",
    "\n",
    "def toggle_pause():\n",
    "    global music_paused\n",
    "    if music_playing:\n",
    "        if not music_paused:\n",
    "            pygame.mixer.music.pause()\n",
    "            status_var.set(\"‚è∏ Music Paused\")\n",
    "            song_var.set(\"Paused: \" + current_song)\n",
    "            music_paused = True\n",
    "        else:\n",
    "            pygame.mixer.music.unpause()\n",
    "            status_var.set(\"‚ñ∂ Music Resumed\")\n",
    "            song_var.set(\"Playing: \" + current_song)\n",
    "            music_paused = False\n",
    "\n",
    "def adjust_volume(change):\n",
    "    global current_volume\n",
    "    current_volume = max(0.0, min(1.0, current_volume + change))\n",
    "    pygame.mixer.music.set_volume(current_volume)\n",
    "    volume_var.set(str(int(current_volume * 100)))\n",
    "    volume_scale.set(int(current_volume * 100))\n",
    "\n",
    "def set_volume_from_scale(val):\n",
    "    global current_volume\n",
    "    current_volume = float(val) / 100\n",
    "    pygame.mixer.music.set_volume(current_volume)\n",
    "\n",
    "def toggle_gesture_detection():\n",
    "    global gesture_detection_enabled\n",
    "    gesture_detection_enabled = not gesture_detection_enabled\n",
    "    if gesture_detection_enabled:\n",
    "        gesture_button.config(text=\"Disable Gesture Control\", bg=\"#f44336\")\n",
    "        status_var.set(\"Gesture control enabled\")\n",
    "    else:\n",
    "        gesture_button.config(text=\"Enable Gesture Control\", bg=\"#4CAF50\")\n",
    "        status_var.set(\"Gesture control disabled\")\n",
    "\n",
    "def start_detection_and_play_song():\n",
    "    detect_mood()\n",
    "    play_song(detected_mood)\n",
    "    if not hasattr(start_detection_and_play_song, 'gesture_thread_started'):\n",
    "        gesture_thread = Thread(target=process_hand_gestures)\n",
    "        gesture_thread.daemon = True\n",
    "        gesture_thread.start()\n",
    "        start_detection_and_play_song.gesture_thread_started = True\n",
    "\n",
    "def start_gui():\n",
    "    global root, status_var, mood_var, song_var, volume_var, mood_label, volume_scale, gesture_button, exit_flag\n",
    "    root = Tk()\n",
    "    root.title(\"Advanced Mood-Based Song Player with Gesture Control\")\n",
    "    root.geometry(\"700x600\")\n",
    "    root.configure(bg=\"#f5f5f5\")\n",
    "\n",
    "    status_var = StringVar()\n",
    "    mood_var = StringVar()\n",
    "    song_var = StringVar()\n",
    "    volume_var = StringVar()\n",
    "    volume_var.set(\"50\")\n",
    "\n",
    "    header_frame = Frame(root, bg=\"#2c3e50\", padx=10, pady=10)\n",
    "    header_frame.pack(fill=\"x\")\n",
    "    Label(header_frame, text=\"üéß Mood-Based Music Player\",\n",
    "          font=(\"Helvetica\", 22, \"bold\"), fg=\"white\", bg=\"#2c3e50\").pack()\n",
    "    Label(header_frame, text=\"Control your music with hand gestures!\",\n",
    "          font=(\"Helvetica\", 12), fg=\"white\", bg=\"#2c3e50\").pack()\n",
    "\n",
    "    status_frame = Frame(root, bg=\"#ecf0f1\", padx=10, pady=10)\n",
    "    status_frame.pack(fill=\"x\")\n",
    "    Label(status_frame, text=\"STATUS\", font=(\"Helvetica\", 12, \"bold\"), bg=\"#ecf0f1\").pack(anchor=\"w\")\n",
    "    Label(status_frame, textvariable=status_var, font=(\"Helvetica\", 10), bg=\"#ecf0f1\", fg=\"#e74c3c\").pack(anchor=\"w\")\n",
    "\n",
    "    mood_frame = Frame(root, bg=\"#f5f5f5\", padx=10, pady=10)\n",
    "    mood_frame.pack(fill=\"x\")\n",
    "    Label(mood_frame, text=\"DETECTED MOOD\", font=(\"Helvetica\", 12, \"bold\"), bg=\"#f5f5f5\").pack(anchor=\"w\")\n",
    "    mood_label = Label(mood_frame, textvariable=mood_var, font=(\"Helvetica\", 16, \"bold\"), bg=\"#f5f5f5\")\n",
    "    mood_label.pack(anchor=\"w\")\n",
    "\n",
    "    song_frame = Frame(root, bg=\"#f5f5f5\", padx=10, pady=10)\n",
    "    song_frame.pack(fill=\"x\")\n",
    "    Label(song_frame, text=\"NOW PLAYING\", font=(\"Helvetica\", 12, \"bold\"), bg=\"#f5f5f5\").pack(anchor=\"w\")\n",
    "    Label(song_frame, textvariable=song_var, font=(\"Helvetica\", 14), bg=\"#f5f5f5\").pack(anchor=\"w\")\n",
    "\n",
    "    volume_frame = Frame(root, bg=\"#f5f5f5\", padx=10, pady=10)\n",
    "    volume_frame.pack(fill=\"x\")\n",
    "    Label(volume_frame, text=\"VOLUME\", font=(\"Helvetica\", 12, \"bold\"), bg=\"#f5f5f5\").pack(anchor=\"w\")\n",
    "    volume_control_frame = Frame(volume_frame, bg=\"#f5f5f5\")\n",
    "    volume_control_frame.pack(fill=\"x\")\n",
    "    volume_scale = Scale(volume_control_frame, from_=0, to=100, orient=HORIZONTAL,\n",
    "                        length=350, command=set_volume_from_scale, variable=volume_var,\n",
    "                        bg=\"#f5f5f5\", troughcolor=\"#3498db\", highlightthickness=0)\n",
    "    volume_scale.pack(side=\"left\")\n",
    "    Label(volume_control_frame, textvariable=volume_var, font=(\"Helvetica\", 12),\n",
    "          bg=\"#f5f5f5\", width=3).pack(side=\"left\", padx=10)\n",
    "\n",
    "    control_frame = Frame(root, bg=\"#f5f5f5\", padx=10, pady=10)\n",
    "    control_frame.pack(fill=\"x\")\n",
    "    Button(control_frame, text=\"Detect Mood & Play\", command=start_detection_and_play_song,\n",
    "           bg=\"#3498db\", fg=\"white\", font=(\"Helvetica\", 12),\n",
    "           padx=20, pady=5).pack(side=\"left\", padx=5)\n",
    "    Button(control_frame, text=\"Stop\", command=stop_song,\n",
    "           bg=\"#e74c3c\", fg=\"white\", font=(\"Helvetica\", 12),\n",
    "           padx=20, pady=5).pack(side=\"left\", padx=5)\n",
    "    Button(control_frame, text=\"Next Song\", command=next_song,\n",
    "           bg=\"#2ecc71\", fg=\"white\", font=(\"Helvetica\", 12),\n",
    "           padx=20, pady=5).pack(side=\"left\", padx=5)\n",
    "    Button(control_frame, text=\"Pause/Play\", command=toggle_pause,\n",
    "           bg=\"#f39c12\", fg=\"white\", font=(\"Helvetica\", 12),\n",
    "           padx=20, pady=5).pack(side=\"left\", padx=5)\n",
    "\n",
    "    gesture_frame = Frame(root, bg=\"#f5f5f5\", padx=10, pady=10)\n",
    "    gesture_frame.pack(fill=\"x\")\n",
    "    gesture_button = Button(gesture_frame, text=\"Disable Gesture Control\", command=toggle_gesture_detection,\n",
    "                           bg=\"#f44336\", fg=\"white\", font=(\"Helvetica\", 12),\n",
    "                           padx=20, pady=5)\n",
    "    gesture_button.pack()\n",
    "\n",
    "    instruction_frame = Frame(root, bg=\"#f5f5f5\", padx=10, pady=10)\n",
    "    instruction_frame.pack(fill=\"x\", expand=True)\n",
    "    instruction_text = \"\"\"\n",
    "    HOW TO USE:\n",
    "\n",
    "    1. Click \"Detect Mood & Play\" to analyze your mood and start playing appropriate music\n",
    "    2. Use hand gestures to control playback:\n",
    "       - Palm facing camera: Stop music\n",
    "       - Thumbs up: Play music\n",
    "       - Fist: Pause/Resume\n",
    "       - Swipe left/right: Change song\n",
    "       - Pinch in/out: Adjust volume\n",
    "    3. Or use the buttons for manual control\n",
    "\n",
    "    Note: Make sure you have songs in your songs folder and a webcam connected!\n",
    "    \"\"\"\n",
    "    Label(instruction_frame, text=instruction_text, justify=\"left\",\n",
    "          font=(\"Helvetica\", 10), bg=\"#f5f5f5\", anchor=\"w\").pack(fill=\"both\")\n",
    "\n",
    "    footer_frame = Frame(root, bg=\"#2c3e50\", padx=10, pady=5)\n",
    "    footer_frame.pack(fill=\"x\", side=\"bottom\")\n",
    "    Label(footer_frame, text=\"¬© 2023 Music Mood Player - Hand Gesture Controlled\",\n",
    "          font=(\"Helvetica\", 8), fg=\"white\", bg=\"#2c3e50\").pack()\n",
    "\n",
    "    status_var.set(\"Ready! Click 'Detect Mood & Play' to start\")\n",
    "    mood_var.set(\"Mood: Not detected\")\n",
    "    song_var.set(\"No song playing\")\n",
    "\n",
    "    def on_closing():\n",
    "        global exit_flag\n",
    "        exit_flag = True\n",
    "        root.destroy()\n",
    "\n",
    "    root.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "    root.mainloop()\n",
    "\n",
    "# Initialize and run\n",
    "pygame.mixer.music.set_volume(current_volume)\n",
    "if not os.path.exists(song_folder):\n",
    "    print(f\"Warning: Song folder '{song_folder}' not found. Creating it...\")\n",
    "    os.makedirs(song_folder)\n",
    "    print(f\"Please add MP3 files to {song_folder}\")\n",
    "\n",
    "start_gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd94564-6544-4d80-9679-550f6ff9d7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
